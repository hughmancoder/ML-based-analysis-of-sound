{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e9c14a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42babf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/hughsignoriello/Developer/ML-based-analysis-of-sound\n"
     ]
    }
   ],
   "source": [
    "# If the notebook lives in notebooks/, hop to repo root so relative paths work\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "if (Path.cwd() / \"data\").exists() is False and (Path.cwd().name == \"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b20926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from train_irmas import SimpleMelNpyDataset, pad_time_axes\n",
    "\n",
    "# If CNNVarTime is in CNN.py next to the notebook:\n",
    "from CNN import CNNVarTime   # or paste the class inline in a separate cell\n",
    "\n",
    "# Repro\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# Paths / hyperparams\n",
    "TRAIN_MANIFEST = \"data/manifests/irmas_train_mels.csv\"\n",
    "CKPT_DIR = Path(\"saved_weights/irmas_pretrain\"); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPOCHS       = 30\n",
    "BATCH_SIZE   = 32\n",
    "LR           = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "VAL_FRAC     = 0.15\n",
    "DROPOUT      = 0.5\n",
    "PATIENCE     = 8\n",
    "\n",
    "# TODO: modify \n",
    "NUM_WORKERS  = 0   # set 0 (or 1). You can raise later after moving dataset/collate to a .py\n",
    "\n",
    "# Device & AMP\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"; USE_CUDA_AMP = False; USE_MPS_AMP = True\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"; USE_CUDA_AMP = True; USE_MPS_AMP = False\n",
    "else:\n",
    "    DEVICE = \"cpu\";  USE_CUDA_AMP = False; USE_MPS_AMP = False\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0dc396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch: 1\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNVarTime(in_ch=2, num_classes=11).to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(\"saved_weights/irmas_pretrain/best_val_acc.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "label_to_idx = ckpt.get(\"label_to_idx\", None)  # handy for evaluation\n",
    "epochs_loaded = ckpt.get(\"epoch\")\n",
    "print(\"Loaded epoch:\", epochs_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cfa43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'cel': 0, 'cla': 1, 'flu': 2, 'gac': 3, 'gel': 4, 'org': 5, 'pia': 6, 'sax': 7, 'tru': 8, 'vio': 9, 'voi': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5699, 1006)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build label map once\n",
    "tmp_ds = SimpleMelNpyDataset(TRAIN_MANIFEST, train=True, per_example_norm=True)\n",
    "label_to_idx = tmp_ds.label_to_idx\n",
    "NUM_CLASSES = len(label_to_idx)\n",
    "print(\"Classes:\", label_to_idx)\n",
    "\n",
    "# Full training ds (no heavy aug to keep it minimal & reproducible)\n",
    "full_ds = SimpleMelNpyDataset(TRAIN_MANIFEST, label_to_idx=label_to_idx, train=True, per_example_norm=True)\n",
    "\n",
    "# Split\n",
    "N = len(full_ds); n_val = int(round(N * VAL_FRAC)); n_train = N - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "val_ds.dataset.train = False\n",
    "\n",
    "# DataLoaders\n",
    "pin_mem = (DEVICE == \"cuda\")\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=pin_mem, collate_fn=pad_time_axes)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=pin_mem, collate_fn=pad_time_axes)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, optimizer, scheduler, criterion, (optional) resume ---\n",
    "\n",
    "# If you want to start from a saved checkpoint, set this:\n",
    "RESUME_CKPT = \"saved_weights/irmas_pretrain/val_0.56.pt\"  # \"\" to train from scratch\n",
    "\n",
    "# (Optional) resume optimizer/scheduler states too (continue same run)\n",
    "RESUME_ALL  = False  # True to resume optimizer + scheduler\n",
    "\n",
    "# Instantiate model with current NUM_CLASSES\n",
    "model = CNNVarTime(in_ch=2, num_classes=NUM_CLASSES, p_drop=DROPOUT).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if RESUME_CKPT:\n",
    "    ckpt = torch.load(RESUME_CKPT, map_location=DEVICE)\n",
    "\n",
    "    # If the checkpoint carries a label mapping, prefer to use it to avoid class-index drift\n",
    "    ckpt_l2i = ckpt.get(\"label_to_idx\")\n",
    "    if ckpt_l2i is not None and ckpt_l2i != label_to_idx:\n",
    "        # Reconcile: adopt the checkpoint's mapping and update NUM_CLASSES if needed\n",
    "        label_to_idx = ckpt_l2i\n",
    "        NUM_CLASSES = len(label_to_idx)\n",
    "        # If NUM_CLASSES changed, rebuild the model head accordingly\n",
    "        model = CNNVarTime(in_ch=2, num_classes=NUM_CLASSES, p_drop=DROPOUT).to(DEVICE)\n",
    "\n",
    "    # Load model weights (strict=True ensures shapes match)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])  # if you changed head size, set strict=False and handle missing keys\n",
    "\n",
    "    if RESUME_ALL:\n",
    "        if \"opt_state\" in ckpt:\n",
    "            optimizer.load_state_dict(ckpt[\"opt_state\"])\n",
    "        if \"sched_state\" in ckpt:\n",
    "            scheduler.load_state_dict(ckpt[\"sched_state\"])\n",
    "        print(f\"Full resume from: {RESUME_CKPT} | epoch: {ckpt.get('epoch')}\")\n",
    "    else:\n",
    "        print(f\"Warm-started from: {RESUME_CKPT} (model weights only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2701effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9g/kj2r9l693tgcs5y__cjlbp4r0000gn/T/ipykernel_30383/523593545.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_cuda_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/30] train 2.3378/0.1561 | val 2.1618/0.3002 | time 16.9s\n",
      "[002/30] train 2.2036/0.2146 | val 2.0235/0.3178 | time 14.4s\n",
      "[003/30] train 2.1162/0.2381 | val 1.9496/0.3405 | time 13.3s\n",
      "[004/30] train 2.0564/0.2690 | val 1.9564/0.3026 | time 15.0s\n",
      "[005/30] train 1.9994/0.2812 | val 1.8276/0.3764 | time 15.6s\n",
      "[006/30] train 1.9630/0.2937 | val 1.8111/0.3825 | time 16.0s\n",
      "[007/30] train 1.9158/0.3155 | val 1.8093/0.3624 | time 13.3s\n",
      "[008/30] train 1.8928/0.3275 | val 1.6974/0.4347 | time 16.0s\n",
      "[009/30] train 1.8448/0.3492 | val 1.6665/0.4282 | time 15.3s\n",
      "[010/30] train 1.8330/0.3538 | val 1.6632/0.4318 | time 15.1s\n",
      "[011/30] train 1.8032/0.3626 | val 1.6109/0.4708 | time 15.1s\n",
      "[012/30] train 1.7785/0.3733 | val 1.5989/0.4787 | time 16.2s\n",
      "[013/30] train 1.7513/0.3893 | val 1.6041/0.4637 | time 15.1s\n",
      "[014/30] train 1.7234/0.4011 | val 1.5310/0.4904 | time 15.5s\n",
      "[015/30] train 1.7088/0.4014 | val 1.4999/0.5151 | time 13.1s\n",
      "[016/30] train 1.6954/0.4004 | val 1.4812/0.5170 | time 14.6s\n",
      "[017/30] train 1.6944/0.4096 | val 1.4949/0.5160 | time 15.2s\n",
      "[018/30] train 1.6736/0.4223 | val 1.4668/0.5102 | time 15.2s\n",
      "[019/30] train 1.6598/0.4299 | val 1.4644/0.5258 | time 14.6s\n",
      "[020/30] train 1.6430/0.4269 | val 1.4594/0.5278 | time 13.9s\n",
      "[021/30] train 1.6427/0.4360 | val 1.4266/0.5388 | time 13.4s\n",
      "[022/30] train 1.6295/0.4353 | val 1.4411/0.5405 | time 13.1s\n",
      "[023/30] train 1.6246/0.4370 | val 1.4226/0.5427 | time 15.6s\n",
      "[024/30] train 1.6050/0.4426 | val 1.4231/0.5492 | time 15.8s\n",
      "[025/30] train 1.6333/0.4360 | val 1.4222/0.5434 | time 16.2s\n",
      "[026/30] train 1.6243/0.4359 | val 1.4100/0.5476 | time 16.5s\n",
      "[027/30] train 1.6185/0.4408 | val 1.4133/0.5453 | time 14.7s\n",
      "[028/30] train 1.6237/0.4331 | val 1.4205/0.5385 | time 14.9s\n",
      "[029/30] train 1.6084/0.4396 | val 1.4148/0.5424 | time 13.0s\n",
      "[030/30] train 1.5930/0.4494 | val 1.4103/0.5466 | time 14.1s\n",
      "Best val acc: 0.5492466520518064\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "no_improve = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "use_cuda_amp = (DEVICE == \"cuda\")\n",
    "use_mps_amp  = (DEVICE == \"mps\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_cuda_amp)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    tr_loss = tr_acc = nb = 0\n",
    "    for X, y, _ in train_loader:\n",
    "        X = X.to(DEVICE, non_blocking=pin_mem); y = y.to(DEVICE, non_blocking=pin_mem)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_cuda_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(X); loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            if use_mps_amp:\n",
    "                with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "                    logits = model(X); loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = model(X); loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_acc  += (logits.argmax(1) == y).float().mean().item()\n",
    "        nb += 1\n",
    "    tr_loss /= max(1, nb); tr_acc /= max(1, nb)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    va_loss = va_acc = nb = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _ in val_loader:\n",
    "            X = X.to(DEVICE, non_blocking=pin_mem); y = y.to(DEVICE, non_blocking=pin_mem)\n",
    "            if use_cuda_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(X); loss = criterion(logits, y)\n",
    "            else:\n",
    "                if use_mps_amp:\n",
    "                    with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "                        logits = model(X); loss = criterion(logits, y)\n",
    "                else:\n",
    "                    logits = model(X); loss = criterion(logits, y)\n",
    "            va_loss += loss.item()\n",
    "            va_acc  += (logits.argmax(1) == y).float().mean().item()\n",
    "            nb += 1\n",
    "    va_loss /= max(1, nb); va_acc /= max(1, nb)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"[{epoch:03d}/{EPOCHS}] train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # Save \"last\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"opt_state\": optimizer.state_dict(),\n",
    "        \"sched_state\": scheduler.state_dict(),\n",
    "        \"label_to_idx\": label_to_idx,\n",
    "        \"history\": history,\n",
    "    }, CKPT_DIR / \"last.pt\")\n",
    "\n",
    "    # Save \"best\"\n",
    "    if va_acc > best_val_acc + 1e-6:\n",
    "        best_val_acc = va_acc; no_improve = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": optimizer.state_dict(),\n",
    "            \"sched_state\": scheduler.state_dict(),\n",
    "            \"label_to_idx\": label_to_idx,\n",
    "            \"history\": history,\n",
    "        }, CKPT_DIR / \"best_val_acc.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val acc {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "print(\"Best val acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend(); plt.title(\"Loss\"); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend(); plt.title(\"Accuracy\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
