{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4d04fb",
   "metadata": {},
   "source": [
    "# IRMAS Test Evaluation\n",
    "\n",
    "Load the trained `CNNVarTime` checkpoint, run it on the precomputed IRMAS test mel windows, aggregate predictions back to the clip level, and report clip-level accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cb4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "from src.models import CNNVarTime\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "IRMAS_TEST_MANIFEST = PROJECT_ROOT / \"data/manifests/irmas_test_mels.csv\"\n",
    "RESUME_CKPT = PROJECT_ROOT / \"saved_weights/irmas_pretrain/best_val_acc.pt\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e8cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weight mean/std: -0.0013071876019239426 0.11860577017068863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNVarTime(\n",
       "  (conv1): Conv2d(2, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=500, bias=True)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=500, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.checkpoint import class_names_from_ckpt, load_model_state\n",
    "from src.utils.datasets import IRMASTestWindowDataset\n",
    "from src.utils.utils import CLASSES, pick_device\n",
    "\n",
    "# Align class order with training\n",
    "train_order_names, _ = class_names_from_ckpt(RESUME_CKPT, fallback=list(CLASSES))\n",
    "test_dataset = IRMASTestWindowDataset(\n",
    "    manifest_csv=IRMAS_TEST_MANIFEST,\n",
    "    project_root=PROJECT_ROOT,\n",
    "    class_names=train_order_names,\n",
    "    per_example_norm=True,   # must match your train setting\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=None,\n",
    "    collate_fn=None,\n",
    ")\n",
    "\n",
    "DEVICE = pick_device()\n",
    "# Build model + strict load\n",
    "model = CNNVarTime(num_classes=len(train_order_names)).to(DEVICE)\n",
    "state_dict = load_model_state(RESUME_CKPT)\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=True)\n",
    "assert not missing and not unexpected, f\"state_dict mismatch: {missing} | {unexpected}\"\n",
    "w = model.state_dict()[\"conv1.weight\"]\n",
    "print(\"conv1 weight mean/std:\", float(w.mean()), float(w.std()))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a514910f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\src\\utils\\datasets.py\", line 80, in __getitem__\n    raise FileNotFoundError(mel_path)\nFileNotFoundError: e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.cache\\mels\\irmas\\test\\01 - Canto das três raças-10__5090e7f4ec__sr44100_dur3.0_m128_w30_h10_s0.npy\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m pin_memory = (DEVICE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# (B, C)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\src\\utils\\datasets.py\", line 80, in __getitem__\n    raise FileNotFoundError(mel_path)\nFileNotFoundError: e:\\qingchaolaopian\\Instrument Sound\\Github\\ML-based-analysis-of-sound\\.cache\\mels\\irmas\\test\\01 - Canto das três raças-10__5090e7f4ec__sr44100_dur3.0_m128_w30_h10_s0.npy\n"
     ]
    }
   ],
   "source": [
    "logits_by_clip = defaultdict(list)\n",
    "labels_by_clip = {}\n",
    "windows_per_clip = defaultdict(int)\n",
    "pin_memory = (DEVICE == \"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, clip_ids, paths in test_loader:\n",
    "        inputs = inputs.to(DEVICE, non_blocking=pin_memory)\n",
    "        logits = model(inputs)                         # (B, C)\n",
    "        logits_cpu = logits.detach().cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "\n",
    "        for logit_vec, target_vec, clip_id in zip(logits_cpu, targets_np, clip_ids):\n",
    "            logits_by_clip[clip_id].append(logit_vec)\n",
    "            labels_by_clip[clip_id] = target_vec.astype(np.float32)\n",
    "            windows_per_clip[clip_id] += 1\n",
    "\n",
    "# --- aggregate per clip & score single-label membership ---\n",
    "rows = []\n",
    "for clip_id, logit_list in logits_by_clip.items():\n",
    "    stacked = np.stack(logit_list, 0)                # (W, C)\n",
    "    mean_logits = stacked.mean(axis=0)               # (C,)\n",
    "    exp = np.exp(mean_logits - mean_logits.max())\n",
    "    probs = exp / exp.sum()\n",
    "    pred_idx = int(mean_logits.argmax())\n",
    "    gt_vec = labels_by_clip[clip_id]\n",
    "    hit1 = bool(gt_vec[pred_idx] > 0.5)\n",
    "\n",
    "    order = np.argsort(mean_logits)[::-1]\n",
    "    top3 = \", \".join(f\"{train_order_names[i]} ({probs[i]:.2f})\" for i in order[:3])\n",
    "    true_names = [train_order_names[i] for i, v in enumerate(gt_vec) if v > 0.5]\n",
    "\n",
    "    rows.append({\n",
    "        \"clip\": clip_id,\n",
    "        \"true_labels\": \"|\".join(true_names),\n",
    "        \"pred_label\": train_order_names[pred_idx],\n",
    "        \"pred_score\": float(probs[pred_idx]),\n",
    "        \"top3\": top3,\n",
    "        \"hit@1\": hit1,\n",
    "        \"windows\": int(windows_per_clip[clip_id]),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"hit@1\", \"pred_score\"], ascending=[True, False])\n",
    "print(\"Top-1 clip accuracy:\", df[\"hit@1\"].mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class = []\n",
    "for ci, cname in enumerate(train_order_names):\n",
    "    mask = df[\"true_labels\"].str.contains(rf\"(?:^|[|]){cname}(?:$|[|])\")\n",
    "    support = int(mask.sum())\n",
    "    correct = int((df.loc[mask, \"pred_label\"] == cname).sum())\n",
    "    per_class.append({\"class\": cname, \"support\": support, \"hit_rate\": (correct / support if support else float(\"nan\"))})\n",
    "per_class_df = pd.DataFrame(per_class).sort_values(\"hit_rate\", ascending=False)\n",
    "per_class_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
